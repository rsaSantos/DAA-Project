{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "#### João:\n",
    "- [x] cena das datas passar horas para tarde/noite/manha/madrugada && dividr dias do mes e mes // Semi Done not sure what to do with month and days divider\n",
    "- [x] nominal to numeric | NOminal value discretization using label Encoding\n",
    "- [] outliers\n",
    "- [] noise  NOTE: Binning is a technique for data smoothing. Data smoothing is employed to removenoise from data.\n",
    "- [x] duplicated Records \n",
    "\n",
    "#### Jorge\n",
    "- [x] Binning\n",
    "- [x] feature scalling\n",
    "- [x] svn\n",
    "- [x] training and testing data\n",
    "\n",
    "# Dados e Aprendizagem Automática\n",
    "## Trabalho Prático\n",
    "### Grupo 13\n",
    "### Membros:\n",
    "- (PG50304) Cristiano Pereira\n",
    "- (PG50463) João Martins\n",
    "- (PG50506) Jorge Lima\n",
    "- (PG50733) Rúben Santos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook structure:\n",
    "- 1. Importing libraries\n",
    "- 2. Loading the dataset\n",
    "- 3. Exploratory Data Analysis TODO Joao\n",
    "- 4. Data Preprocessing TODO Joao\n",
    "- 5. Model Training TODO Jorge\n",
    "- 6. Model Evaluation TODO Jorge\n",
    "- 8. Conclusion Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, tree, metrics, model_selection, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# Lois\n",
    "from lois import lois_ds_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv('data/training_data.csv')\n",
    "testData = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis TODO Joao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lois_ds_report(trainingData,target_variable=\"incidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing TODO Joao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for any Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingData[trainingData.duplicated()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collum affected_roles to numeric\n",
    "\n",
    "def count_roads(affected_roads):\n",
    "    #check if affected_roads is a float(nan values)\n",
    "    if isinstance(affected_roads, float) and np.isnan(affected_roads):\n",
    "        return 0\n",
    "    return len(affected_roads.split(','))\n",
    "\n",
    "# count the number of different roads affected\n",
    "def count_roads(affected_roads):\n",
    "    #check if affected_roads is a float(nan values)\n",
    "    if isinstance(affected_roads, float) and np.isnan(affected_roads):\n",
    "        return 0\n",
    "    result = []\n",
    "    return len([result.append(x) for x in affected_roads.split(',') if x not in result] )\n",
    "\n",
    "\n",
    "trainingData['number_affected_roads'] = trainingData['affected_roads'].apply(count_roads)\n",
    "testData['number_affected_roads'] = testData['affected_roads'].apply(count_roads)\n",
    "trainingData['number_different_affected_roads'] = trainingData['affected_roads'].apply(count_roads)\n",
    "testData['number_different_affected_roads'] = testData['affected_roads'].apply(count_roads)\n",
    "\n",
    "print(trainingData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing if exists null values\n",
    "# TODO: Coorect to drop the affected_roads collumn?\n",
    "trainingData.drop('affected_roads', axis=1, inplace=True)\n",
    "testData.drop('affected_roads', axis=1, inplace=True)\n",
    "trainingData.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.profile_report(minimal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping city_name because it's only data from Guimaraes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove city_name column, only guimaraes is present\n",
    "trainingData.drop('city_name',axis=1,inplace=True)\n",
    "testData.drop('city_name',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn values to Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Turn nominal values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_magnitude_of_delay = {'UNDEFINED': 0, 'MODERATE':1, 'MAJOR':3}\n",
    "trainingData['magnitude_of_delay'] = trainingData['magnitude_of_delay'].map(dict_magnitude_of_delay)\n",
    "testData['magnitude_of_delay'] = testData['magnitude_of_delay'].map(dict_magnitude_of_delay)\n",
    "\n",
    "dict_luminosity = {'LOW_LIGHT': 0, 'LIGHT':1, 'DARK':2}\n",
    "trainingData['luminosity'] = trainingData['luminosity'].map(dict_luminosity)\n",
    "testData['luminosity'] = testData['luminosity'].map(dict_luminosity)\n",
    "\n",
    "dict_avg_rain = {'Sem Chuva': 0, 'chuva fraca': 1, 'chuva moderada': 2, 'chuva forte': 3}\n",
    "trainingData['avg_rain'] = trainingData['avg_rain'].map(dict_avg_rain)\n",
    "testData['avg_rain'] = testData['avg_rain'].map(dict_avg_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorizing time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn column record_date into manha,tarde,noite or madrugada\n",
    "def turn_date_into_time_of_day(record_date):\n",
    "    hour = int(record_date.split(' ')[1].split(':')[0])\n",
    "    if hour >= 6 and hour < 12:\n",
    "        return 'manha'\n",
    "    elif hour >= 12 and hour < 18:\n",
    "        return 'tarde'\n",
    "    elif hour >= 18 and hour < 24:\n",
    "        return 'noite'\n",
    "    else:\n",
    "        return 'madrugada'\n",
    "\n",
    "\n",
    "trainingData['time_of_day'] = trainingData['record_date'].apply(turn_date_into_time_of_day)\n",
    "testData['time_of_day'] = testData['record_date'].apply(turn_date_into_time_of_day)\n",
    "\n",
    "dict_time_of_day = {'manha': 0, 'tarde':1, 'noite':2, 'madrugada':3}\n",
    "trainingData['time_of_day'] = trainingData['time_of_day'].map(dict_time_of_day)\n",
    "testData['time_of_day'] = testData['time_of_day'].map(dict_time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 2 column with the month and day from time_of_day\n",
    "trainingData['month'] = pd.DatetimeIndex(trainingData['record_date']).month\n",
    "testData['month'] = pd.DatetimeIndex(testData['record_date']).month\n",
    "trainingData['day'] = pd.DatetimeIndex(trainingData['record_date']).day\n",
    "testData['day'] = pd.DatetimeIndex(testData['record_date']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = trainingData.corr(numeric_only=True)\n",
    "sns.heatmap(corr_matrix,vmin=1,vmax=1,square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def apply_binning(data, column, n_bins, encode='ordinal', strategy='quantile'):\n",
    "    if column in data.columns:\n",
    "        estimater = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n",
    "        data[column + \"_binned\"] = estimater.fit_transform(data[[column]])\n",
    "        data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#apply_binning(testingData, 'avg_atm_pressure', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def apply_normalization(data, column):\n",
    "    if column in data.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[column + \"_normalized\"] = scaler.fit_transform(data[[column]])\n",
    "        data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#apply_normalization(testingData, 'delay_in_seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_standardization(data, column):\n",
    "    if column in data.columns:\n",
    "        scaler = StandardScaler()\n",
    "        data[column + \"_standardized\"] = scaler.fit_transform(data[[column]])\n",
    "        data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#apply_standardization(testingData, 'avg_humidity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models Training TODO Jorge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save results to a file. \"\"\"\n",
    "def saveResults ( results, path ):\n",
    "    file = open(path, 'w')\n",
    "    file.write (\"RowId,Incidents\\n\")\n",
    "    i = 1\n",
    "    for result in results :\n",
    "        file.write (str(i) + \",\" + result + \"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for test or run for submission\n",
    "results_to_test = True\n",
    "\n",
    "# split data into train and test\n",
    "trainData = trainingData.copy()\n",
    "ttData = testData.copy()\n",
    "\n",
    "if results_to_test:\n",
    "    y = trainData['incidents']\n",
    "    x = trainData.drop(['incidents'], axis=1)\n",
    "    x = x.drop(['record_date'], axis=1) # TODO REMOVER\n",
    "    x_training, x_testing, y_training, y_testing = model_selection.train_test_split(x, y, test_size=0.5, random_state=2022)\n",
    "\n",
    "else :\n",
    "    x_testing = ttData.drop(['record_date'], axis=1) # TODO REMOVER\n",
    "    y_training = trainData['incidents'] \n",
    "    x_training = trainData.drop(['incidents'], axis=1)\n",
    "    x_training = x_training.drop(['record_date'], axis=1) # TODO REMOVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = linear_model.LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "logmodel.fit(x_training,y_training)\n",
    "logistic_regression_predictions = logmodel.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Árvores de Decisão e Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(random_state=2022, criterion=\"gini\", max_depth=10)\n",
    "dtc.fit(x_training,y_training)\n",
    "decision_trees_predictions = dtc.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Maquinas de Vectores de Suporte (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(x_training, columns=x_training.columns)\n",
    "df_target = pd.DataFrame(y_training, columns=['incidents'])\n",
    "\n",
    "# 10-Fold Cross Validation\n",
    "cross_valid_model = SVC(random_state=2022)\n",
    "cross_val_score(cross_valid_model, df_feat, np.ravel(df_target), cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=2022)\n",
    "model.fit(x_training, y_training)\n",
    "hold_out_predictions = model.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}\n",
    "grid = GridSearchCV(SVC(random_state=2022), param_grid, refit=True, verbose=3)\n",
    "grid.fit(x_training, y_training)\n",
    "grid_predictions = grid.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_training, y_training)\n",
    "knn_predictions = knn.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = GaussianProcessClassifier(n_restarts_optimizer=0, max_iter_predict=100, random_state=2022)\n",
    "gaussian.fit(x_training, y_training)\n",
    "gaussian_predictions = gaussian.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=2022)\n",
    "random_forest.fit(x_training, y_training)\n",
    "random_forest_predictions = random_forest.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpclass = MLPClassifier(alpha=1, max_iter=1000)\n",
    "mlpclass.fit(x_training, y_training)\n",
    "mlpclass_predictions = mlpclass.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaBoost = AdaBoostClassifier(n_estimators=100, random_state=2022)\n",
    "adaBoost.fit(x_training, y_training)\n",
    "adaBoost_predictions = adaBoost.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianNB = GaussianNB()\n",
    "gaussianNB.fit(x_training, y_training)\n",
    "gaussianNB_predictions = gaussianNB.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.12 QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadraticDiscriminantAnalysis = QuadraticDiscriminantAnalysis()\n",
    "quadraticDiscriminantAnalysis.fit(x_training, y_training)\n",
    "quadraticDiscriminantAnalysis_predictions = quadraticDiscriminantAnalysis.predict(x_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation TODO Jorge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_to_test:\n",
    "\n",
    "    results = []\n",
    "    results_table = []\n",
    "\n",
    "    # 5.1 Regressão Logística\n",
    "    logistic_regression_classification = metrics.classification_report(y_testing, logistic_regression_predictions, zero_division=0)\n",
    "    logistic_regression_classification_dict = metrics.classification_report(y_testing, logistic_regression_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Regressão Logística', 'table': logistic_regression_classification})\n",
    "    results.append({\n",
    "        'model': 'Regressão Logística',\n",
    "        'accuracy': logistic_regression_classification_dict['accuracy'],\n",
    "        'precision': logistic_regression_classification_dict['macro avg']['precision'],\n",
    "        'recall': logistic_regression_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': logistic_regression_classification_dict['macro avg']['f1-score'],\n",
    "        })\n",
    "\n",
    "    # 5.2 Decision Tree\n",
    "    decision_trees_classification = metrics.classification_report(y_testing, decision_trees_predictions, zero_division=0)\n",
    "    decision_trees_classification_dict = metrics.classification_report(y_testing, decision_trees_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Decision Tree', 'table': decision_trees_classification})\n",
    "    results.append({\n",
    "        'model': 'Decision Tree',\n",
    "        'accuracy': decision_trees_classification_dict['accuracy'],\n",
    "        'precision': decision_trees_classification_dict['macro avg']['precision'],\n",
    "        'recall': decision_trees_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': decision_trees_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.4 Hold Out\n",
    "    hold_out_classification = metrics.classification_report(y_testing, hold_out_predictions, zero_division=0)\n",
    "    hold_out_classification_dict = metrics.classification_report(y_testing, hold_out_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Hold Out', 'table': hold_out_classification})\n",
    "    results.append({\n",
    "        'model': 'Hold Out',\n",
    "        'accuracy': hold_out_classification_dict['accuracy'],\n",
    "        'precision': hold_out_classification_dict['macro avg']['precision'],\n",
    "        'recall': hold_out_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': hold_out_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.5 Grid Search\n",
    "    grid_classification = metrics.classification_report(y_testing, grid_predictions, zero_division=0)\n",
    "    grid_classification_dict = metrics.classification_report(y_testing, grid_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Grid Search', 'table': grid_classification})\n",
    "    results.append({\n",
    "        'model': 'Grid Search',\n",
    "        'accuracy': grid_classification_dict['accuracy'],\n",
    "        'precision': grid_classification_dict['macro avg']['precision'],\n",
    "        'recall': grid_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': grid_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    #  5.6 KNN\n",
    "    knn_classification = metrics.classification_report(y_testing, knn_predictions, zero_division=0)\n",
    "    knn_classification_dict = metrics.classification_report(y_testing, knn_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'KNN', 'table': knn_classification})\n",
    "    results.append({\n",
    "        'model': 'KNN',\n",
    "        'accuracy': knn_classification_dict['accuracy'],\n",
    "        'precision': knn_classification_dict['macro avg']['precision'],\n",
    "        'recall': knn_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': knn_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.7 Gaussian\n",
    "    gaussian_classification = metrics.classification_report(y_testing, gaussian_predictions, zero_division=0)\n",
    "    gaussian_classification_dict = metrics.classification_report(y_testing, gaussian_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Gaussian', 'table': gaussian_classification})\n",
    "    results.append({\n",
    "        'model': 'Gaussian',\n",
    "        'accuracy': gaussian_classification_dict['accuracy'],\n",
    "        'precision': gaussian_classification_dict['macro avg']['precision'],\n",
    "        'recall': gaussian_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': gaussian_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.8 Random Forest\n",
    "    random_forest_classification = metrics.classification_report(y_testing, random_forest_predictions, zero_division=0)\n",
    "    random_forest_classification_dict = metrics.classification_report(y_testing, random_forest_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Random Forest', 'table': random_forest_classification})\n",
    "    results.append({\n",
    "        'model': 'Random Forest',\n",
    "        'accuracy': random_forest_classification_dict['accuracy'],\n",
    "        'precision': random_forest_classification_dict['macro avg']['precision'],\n",
    "        'recall': random_forest_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': random_forest_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.9 MLP\n",
    "    mlp_classification = metrics.classification_report(y_testing, mlpclass_predictions, zero_division=0)\n",
    "    mlp_classification_dict = metrics.classification_report(y_testing, mlpclass_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'MLP', 'table': mlp_classification})\n",
    "    results.append({\n",
    "        'model': 'MLP',\n",
    "        'accuracy': mlp_classification_dict['accuracy'],\n",
    "        'precision': mlp_classification_dict['macro avg']['precision'],\n",
    "        'recall': mlp_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': mlp_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "    \n",
    "    # 5.10 AdaBoost\n",
    "    adaBoost_classification = metrics.classification_report(y_testing, adaBoost_predictions, zero_division=0)\n",
    "    adaBoost_classification_dict = metrics.classification_report(y_testing, adaBoost_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'AdaBoost', 'table': adaBoost_classification})\n",
    "    results.append({\n",
    "        'model': 'AdaBoost',\n",
    "        'accuracy': adaBoost_classification_dict['accuracy'],\n",
    "        'precision': adaBoost_classification_dict['macro avg']['precision'],\n",
    "        'recall': adaBoost_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': adaBoost_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    # 5.11 GaussianNB\n",
    "    gaussianNB_classification = metrics.classification_report(y_testing, gaussianNB_predictions, zero_division=0)\n",
    "    gaussianNB_classification_dict = metrics.classification_report(y_testing, gaussianNB_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'GaussianNB', 'table': gaussianNB_classification})\n",
    "    results.append({\n",
    "        'model': 'GaussianNB',\n",
    "        'accuracy': gaussianNB_classification_dict['accuracy'],\n",
    "        'precision': gaussianNB_classification_dict['macro avg']['precision'],\n",
    "        'recall': gaussianNB_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': gaussianNB_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "    \n",
    "    # 5.12 Quadratic Discriminant Analysis\n",
    "    qda_classification = metrics.classification_report(y_testing, quadraticDiscriminantAnalysis_predictions, zero_division=0)\n",
    "    qda_classification_dict = metrics.classification_report(y_testing, quadraticDiscriminantAnalysis_predictions, output_dict=True, zero_division=0)\n",
    "    results_table.append({'model': 'Quadratic Discriminant Analysis', 'table': qda_classification})\n",
    "    results.append({\n",
    "        'model': 'Quadratic Discriminant Analysis',\n",
    "        'accuracy': qda_classification_dict['accuracy'],\n",
    "        'precision': qda_classification_dict['macro avg']['precision'],\n",
    "        'recall': qda_classification_dict['macro avg']['recall'],\n",
    "        'f1-score': qda_classification_dict['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "else:\n",
    "    saveResults(logistic_regression_predictions, 'results/logisticRegression.csv')\n",
    "    saveResults(decision_trees_predictions, 'results/classificationDecisionTree.csv')\n",
    "    saveResults(hold_out_predictions, 'results/Hold-Out-SVN.csv')\n",
    "    saveResults(grid_predictions, 'results/GridSearchCV.csv')\n",
    "    saveResults(knn_predictions, 'results/KNN.csv')\n",
    "    saveResults(gaussian_predictions, 'results/Gaussian.csv')\n",
    "    saveResults(random_forest_predictions, 'results/RandomForest.csv')\n",
    "    saveResults(mlpclass_predictions, 'results/MLP.csv')\n",
    "    saveResults(adaBoost_predictions, 'results/AdaBoost.csv')\n",
    "    saveResults(gaussianNB_predictions, 'results/GaussianNB.csv')\n",
    "    saveResults(quadraticDiscriminantAnalysis_predictions, 'results/QuadraticDiscriminantAnalysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_table:\n",
    "    print(\"----------\" + result['model'] + \"----------------------------------------------------------------\")\n",
    "    print(result['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plot = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "for row in results:\n",
    "    results_plot = results_plot.append({\n",
    "        'Model': row['model'],\n",
    "        'Accuracy': row['accuracy'],\n",
    "        'Precision': row['precision'],\n",
    "        'Recall': row['recall'],\n",
    "        'F1-Score': row['f1-score']\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 5), dpi=100)\n",
    "sns.barplot(x=results_plot['Model'] , y=results_plot['Accuracy'], ax=ax, palette='Set2')\n",
    "ax.set_xlabel('Model', fontdict={'color':'black', 'weight':'bold', 'size': 16})\n",
    "ax.set_ylabel('Accuracy', fontdict={'color':'black', 'weight':'bold', 'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 5), dpi=100)\n",
    "sns.barplot(x=results_plot['Model'] , y=results_plot['Precision'], ax=ax, palette='Set2')\n",
    "ax.set_xlabel('Model', fontdict={'color':'black', 'weight':'bold', 'size': 16})\n",
    "ax.set_ylabel('Precision', fontdict={'color':'black', 'weight':'bold', 'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 5), dpi=100)\n",
    "sns.barplot(x=results_plot['Model'] , y=results_plot['Recall'], ax=ax, palette='Set2')\n",
    "ax.set_xlabel('Model', fontdict={'color':'black', 'weight':'bold', 'size': 16})\n",
    "ax.set_ylabel('Recall', fontdict={'color':'black', 'weight':'bold', 'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 5), dpi=100)\n",
    "sns.barplot(x=results_plot['Model'] , y=results_plot['F1-Score'], ax=ax, palette='Set2')\n",
    "ax.set_xlabel('Model', fontdict={'color':'black', 'weight':'bold', 'size': 16})\n",
    "ax.set_ylabel('F1-Score', fontdict={'color':'black', 'weight':'bold', 'size': 16})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('daa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cab99c9c6f49a096354dc2da129958249094ad23e18f1a01ceebabfc3e560501"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
